# Markov
<img align="right" src="logo/logo.png" onerror="this.src = 'assets/logo.png'">

Text generation library based on nth-order Markov chains

![Hex.pm](https://img.shields.io/hexpm/v/markov)
![Hex.pm](https://img.shields.io/hexpm/dw/markov)

## Features
  - **Token sanitation** (optional): ignores letter case and punctuation when
  switching states while still keeping the output as-is
  - **Operation history** (optional): recalls the operations it was instructed
  to perform, incl. past training data
  - **Probability shifting** (optional): gives less frequent generation paths
  more chance to get used, which makes the output more original at the expense
  of being less true to the training data
  - **Tagging** (optional): you can tag your source data and alter the
  probabilities of tagged generation paths according to your rules
  - **Prompted generation** (optional) grants your model the ability to answer
  questions given to it provided that the training data consists mostly of Q&A
  pairs
  - **Managed disk storage** with CubDB so you don't have to worry about storing
  and loading the models

## Usage
In `mix.exs`:
```elixir
defp deps do
  [{:markov, "~> 5.0"}]
end
```

Example workflow (click [here](https://hexdocs.pm/markov/api-reference.html)
for the full documentation):
```elixir
# the model will be stored under the specified path
# as it does not yet exist, it will be created using the specified options
{:ok, model} = Markov.load("./model_path", sanitize_tokens: true, store_log: [:train])

# train using four strings
:ok = Markov.train(model, "hello, world!")
:ok = Markov.train(model, "example string number two")
:ok = Markov.train(model, "hello, Elixir!")
:ok = Markov.train(model, "fourth string")

# generate text
{:ok, text} = Markov.generate_text(model)
IO.puts(text)

# commit all changes and unload
Markov.unload(model)

# the model is unloaded
{:error, _} = Markov.generate_text(model)
{:error, _} = Markov.train(model, "hello, world!")

# load the model again
{:ok, model} = Markov.load("./model_path")

# enable probability shifting and generate text
:ok = Markov.configure(model, shift_probabilities: true)
{:ok, text} = Markov.generate_text(model)
IO.puts(text)

# print log
model |> Markov.read_log |> IO.inspect

# this will also write our new just-set option
Markov.unload(model)
```

## Credits
  - [English dictionary in CSV format](https://www.bragitoff.com/2016/03/english-dictionary-in-csv-format/)

## Model versioning
  - 1.x: required the library user to handle model storage themselves
  - 2.x: introduced managed storage using an in-house database
  - 3.x: switched to mnesia
  - 4.x: switched to [sidx](https://github.com/portasynthinca3/sidx), another
  in-house database
  - 5.x: switched to [CubDB](https://github.com/lucaong/cubdb)

### Migrating from 4.x
Markov 5.x can read log files generated by markov 4.x; thus, if you have enabled
the `store_log: [:train]` option (which is enabled by default), you can generate
a new model based on that data with `Markov.Log.migrate_from_v4/2`:
```elixir
{:ok, model} = Markov.load("/path/to/new/model")
Markov.Log.migrate_from_v4("/path/to/old/model/history.log", model)
Markov.Log.migrate_from_v4("/path/to/old/model/history.log", model, 1000) # limit
```

You may want to do something other to the old log data; for that, use
`Markov.Log.read_v4/1`.
